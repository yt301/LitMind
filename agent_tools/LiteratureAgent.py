from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import FileChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables.config import RunnableConfig
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.agents import initialize_agent, Tool
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from agent_tools.InputModels import TranslateInput, SummaryInput
from agent_tools.TranslationTools import general_translate, academic_translate
from agent_tools.SearchTools import search_tool, literature_search
from agent_tools.SummaryTools import summarize_literature
from langchain.tools import StructuredTool
from agent_tools.Prompts import PROMPT_AGENT_SYSTEM
from config import AGENT_MEMORY_DIR
from pathlib import Path
import os
from typing import cast
import tiktoken

from dotenv import load_dotenv

load_dotenv()

# åˆå§‹åŒ–ç¼–ç å™¨ï¼ˆGPT-3.5/4çš„ç¼–ç ï¼‰
GLOBAL_ENCODER = tiktoken.get_encoding("cl100k_base")


# é‡å†™FileChatMessageHistoryçš„messages å±æ€§ï¼šåªå–æœ€å5æ¡æ¶ˆæ¯ç”¨äºåç»­å¤„ç†ï¼ŒåŒæ—¶é™åˆ¶æœ€å¤§tokenæ•°
class LimitedHistory(FileChatMessageHistory):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.encoder = GLOBAL_ENCODER  # å¼•ç”¨ç¼–ç å™¨

    @property
    def messages(self):
        messages = super().messages  # è·å–å®Œæ•´çš„å¯¹è¯å†å²ï¼ˆä»çˆ¶ç±»FileChatMessageHistoryï¼‰
        limited_messages = []
        total_tokens = 0
        max_tokens = 1500  # å»ºè®®å€¼ï¼šå¯æ ¹æ®ä½ çš„æ¨¡å‹è°ƒæ•´

        # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹åå‘éå†ï¼ˆä¼˜å…ˆä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼‰
        for msg in reversed(messages):
            msg_tokens = len(self.encoder.encode(msg.content))  # ç²¾ç¡®è®¡ç®—å½“å‰æ¶ˆæ¯çš„tokenæ•°
            # msg_tokens = len(msg.content) // 4  # ç®€å•tokenä¼°ç®—ï¼ˆç²¾ç¡®ç‰ˆéœ€è°ƒç”¨tiktokenï¼‰
            if total_tokens + msg_tokens > max_tokens:
                break
            limited_messages.append(msg)
            total_tokens += msg_tokens
            if len(limited_messages) >= 5:  # ç¡¬æ€§æ¡æ•°é™åˆ¶
                break

        return list(reversed(limited_messages))  # æ¢å¤æ—¶é—´é¡ºåº


class LiteratureAgent:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, max_tokens=2000)  # åˆå§‹åŒ–LLMï¼Œè®¾ç½®æ¸©åº¦å’Œæœ€å¤§tokenæ•°
        self.encoder = GLOBAL_ENCODER  # å¼•ç”¨ç¼–ç å™¨
        self._setup_memory()  # æ–°å¢è®°å¿†åˆå§‹åŒ–
        self.tools = [
            StructuredTool.from_function(
                func=lambda text, source_language, translated_language, style: general_translate(
                    self.llm, text, source_language, translated_language, style
                ),
                name="GeneralTranslator",
                description="é€‚åˆæ—¥å¸¸å¯¹è¯ã€éæ­£å¼æ–‡æœ¬çš„æµç•…ç¿»è¯‘ï¼ˆgeneralé£æ ¼ç¿»è¯‘ï¼‰",
                args_schema=TranslateInput,  # å®šä¹‰å‚æ•°çš„ç»“æ„åŒ–è¾“å…¥
                return_direct=False  # ç»•è¿‡Agentçš„ç»“æœå¤„ç†ï¼Œç›´æ¥è¿”å›å·¥å…·è°ƒç”¨ç»“æœ
            ),

            StructuredTool.from_function(
                func=lambda text, source_language, translated_language, style: academic_translate(
                    self.llm, text, source_language, translated_language, style
                ),
                name="AcademicTranslator",
                description="é€‚åˆè®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ä¸¥è°¨åœºæ™¯çš„å­¦æœ¯ç¿»è¯‘ï¼ˆacademicé£æ ¼ç¿»è¯‘ï¼‰",
                args_schema=TranslateInput,  # å®šä¹‰å‚æ•°çš„ç»“æ„åŒ–è¾“å…¥
                return_direct=False  # True:ç»•è¿‡Agentçš„ç»“æœå¤„ç†ï¼Œç›´æ¥è¿”å›å·¥å…·è°ƒç”¨ç»“æœ
            ),

            StructuredTool.from_function(
                func=lambda text, language, detail_level: summarize_literature(
                    self.llm, text, language, detail_level
                ),
                name="LiteratureSummarizer",
                description="å­¦æœ¯æ–‡çŒ®æ€»ç»“å·¥å…·ï¼Œèƒ½å¤Ÿä»æ–‡çŒ®å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç»“æ„åŒ–æ€»ç»“",
                args_schema=SummaryInput,
                return_direct=False
            ),
            literature_search,

        ]
        # self.agent_prompt = hub.pull("hwchase17/openai-tools-agent")
        # åŠ è½½ Agent çš„ Promptï¼ˆLangChain Hub æˆ–è‡ªå®šä¹‰ï¼‰
        self.agent_prompt = ChatPromptTemplate.from_messages([
            ("system", PROMPT_AGENT_SYSTEM),  # ç³»ç»ŸæŒ‡ä»¤ï¼ˆå›ºå®šï¼‰
            ("system", "{history_summary}"),  # æ–°å¢å¯¹è¯å†å²æ€»ç»“æ‘˜è¦
            MessagesPlaceholder("chat_history", optional=True),  # å¯¹è¯å†å²ï¼ˆåŠ¨æ€ï¼‰
            ("human", "{input}"),  # å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆåŠ¨æ€ï¼‰
            MessagesPlaceholder("agent_scratchpad")  # Agentæ‰§è¡Œè¿‡ç¨‹è®°å½•ï¼ˆè‡ªåŠ¨å¡«å……ï¼‰
        ])

        # åˆ›å»º Agent
        self.agent = create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self.agent_prompt
        )
        # åˆ›å»ºä¸å¸¦è®°å¿†çš„Agentæ‰§è¡Œå™¨
        self.agent_executor = AgentExecutor(agent=self.agent,
                                            tools=self.tools,
                                            verbose=True,  # verbose=True å¯ç”¨äºè°ƒè¯•ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—
                                            # max_iterations=1,  # é™åˆ¶åªä½¿ç”¨ä¸€æ¬¡å·¥å…·å‡½æ•°
                                            return_intermediate_steps=True  # å¯ç”¨ä¸­é—´æ­¥éª¤æ•è·ï¼Œèƒ½ç›´æ¥è·å–å·¥å…·è°ƒç”¨ç»“æœ
                                            )
        # åˆ›å»ºå¸¦è®°å¿†çš„Agentæ‰§è¡Œå™¨ï¼ˆRunnableWithMessageHistory+FileChatMessageHistoryï¼‰
        self.agent_memory_executor = RunnableWithMessageHistory(
            AgentExecutor(
                agent=self.agent,
                tools=self.tools,
                verbose=True,
                return_intermediate_steps=True
            ),
            self._get_message_history,  # è®°å¿†è·å–æ–¹æ³•ï¼Œè·å–åŸå§‹å¯¹è¯è®°å½•
            input_messages_key="input",
            history_messages_key="chat_history",
        )

    def _setup_memory(self):
        """åˆå§‹åŒ–è®°å¿†å­˜å‚¨ç›®å½•"""
        self.MEMORY_DIR = Path(AGENT_MEMORY_DIR)
        self.MEMORY_DIR.mkdir(exist_ok=True)

    def _get_message_history(self, session_id: str):
        """è·å–æŒ‡å®šä¼šè¯çš„è®°å¿†"""
        if not session_id:
            raise ValueError("session_idä¸èƒ½ä¸ºç©º")

        file_path = self.MEMORY_DIR / f"{session_id}.json"
        # ç›´æ¥è¿”å›æœ€å10æ¡æ¶ˆæ¯
        return LimitedHistory(str(file_path))
        # return FileChatMessageHistory(str(file_path))

        # return ConversationSummaryBufferMemory(
        #     llm=self.llm,
        #     max_token_limit=1000,  # æ ¹æ®æ¨¡å‹è°ƒæ•´
        #     chat_memory=FileChatMessageHistory(str(file_path)),
        #     memory_key="chat_history",
        #     return_messages=True
        # )

    def _get_summary_memory(self, session_id: str):
        """å§‹åŒ–æ‘˜è¦è®°å¿†"""
        file_path = self.MEMORY_DIR / f"{session_id}.json"
        return ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=1000,  # æœ€å¤šä¿ç•™çš„tokenæ•°é‡ï¼Œæ ¹æ®æ¨¡å‹è°ƒæ•´
            chat_memory=FileChatMessageHistory(str(file_path)),
            memory_key="chat_history",
            return_messages=True
        )

    def _count_tokens(self, text: str) -> int:
        """ä½¿ç”¨tiktokenç²¾ç¡®è®¡ç®—æ–‡æœ¬çš„tokenæ•°"""
        return len(self.encoder.encode(text))

    def _truncate_by_tokens(self, text: str, max_tokens: int) -> str:
        """æŒ‰tokenæ•°ç²¾ç¡®æˆªæ–­æ–‡æœ¬ï¼Œé¿å…æˆªæ–­åŠä¸ªå•è¯æˆ–ä¹±ç 
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_tokens: å…è®¸çš„æœ€å¤§tokenæ•°
        Returns:
            æˆªæ–­åçš„æ–‡æœ¬ï¼ˆtokenæ•° â‰¤ max_tokensï¼‰
        """
        tokens = self.encoder.encode(text)  # ä½¿ç”¨ç±»ä¸­å·²æœ‰çš„encoderï¼ˆtiktokenï¼‰
        if len(tokens) <= max_tokens:
            return text
        return self.encoder.decode(tokens[:max_tokens])  # åªä¿ç•™å‰max_tokensä¸ªtoken

    # ä¸å¸¦è®°å¿†çš„å‡½æ•°æ–¹æ³•

    async def translate(self, text: str, source_language: str, translated_language: str, style: str) -> str:
        """æ–‡çŒ®ç¿»è¯‘å…¥å£æ–¹æ³•ï¼Œæ ¹æ®styleé€‰æ‹©ä¸åŒçš„ç¿»è¯‘å·¥å…·"""
        response = await self.agent_executor.ainvoke({
            "input": {
                "text": text,
                "source_language": source_language,
                "translated_language": translated_language,
                "style": style  # ç¡®ä¿styleå‚æ•°æ˜¾å¼ä¼ é€’
            }
        })

        # æå–æœ€åä¸€æ¬¡å·¥å…·è°ƒç”¨çš„åŸå§‹ç»“æœ
        if response.get("intermediate_steps"):
            last_action, last_result = response["intermediate_steps"][-1]
            if last_action.tool == "GeneralTranslator" or last_action.tool == "AcademicTranslator":
                return last_result  # ç›´æ¥è¿”å›ç¿»è¯‘ç»“æœ
        # å¦‚æœæ²¡æœ‰ä¸­é—´æ­¥éª¤æˆ–æœªè°ƒç”¨å·¥å…·ï¼Œå›é€€åˆ°é»˜è®¤è¾“å‡º
        return response["output"]

    async def summary(self, text: str, language: str, detail_level: str) -> str:
        """æ–‡çŒ®æ€»ç»“å·¥å…·å…¥å£æ–¹æ³•"""
        response = await self.agent_executor.ainvoke({
            "input": {
                "text": text,
                "language": language,
                "detail_level": detail_level  # ç¡®ä¿detail_levelå‚æ•°æ˜¾å¼ä¼ é€’
            },
            "return_intermediate_steps": True  # å¯ç”¨ä¸­é—´æ­¥éª¤æ•è·ï¼Œèƒ½ç›´æ¥è·å–å·¥å…·è°ƒç”¨ç»“æœ
        })

        # æå–æœ€åä¸€æ¬¡å·¥å…·è°ƒç”¨çš„åŸå§‹ç»“æœ
        if response.get("intermediate_steps"):
            last_action, last_result = response["intermediate_steps"][-1]
            if last_action.tool == "LiteratureSummarizer":
                return last_result  # ç›´æ¥è¿”å›æ–‡çŒ®æ€»ç»“ç»“æœ
        # å¦‚æœæ²¡æœ‰ä¸­é—´æ­¥éª¤æˆ–æœªè°ƒç”¨å·¥å…·ï¼Œå›é€€åˆ°é»˜è®¤è¾“å‡º
        return response["output"]

    async def talk(self, user_input: str) -> str:
        """ä¸Agentè¿›è¡Œå¯¹è¯"""
        response = await self.agent_executor.ainvoke({
            "input": user_input,
        })
        # print("Response:", response)  # æ‰“å°å®Œæ•´çš„å“åº”å†…å®¹

        literature_summary = ""
        # å¦‚æœæ˜¯æ–‡çŒ®æ€»ç»“å·¥å…·ï¼Œè®°å½•å·¥å…·è¿”å›ç»“æœ
        if response.get("intermediate_steps"):
            for action, result in response["intermediate_steps"]:
                if action.tool == "LiteratureSummarizer":
                    literature_summary += str(result)
        # print("æ–‡çŒ®æ€»ç»“ç»“æœï¼š", literature_summary)
        final_response = literature_summary + "\n\n" + response["output"]  # å°†æ€»ç»“ç»“æœå’Œæœ€ç»ˆè¾“å‡ºåˆå¹¶
        # print("æœ€ç»ˆè¾“å‡ºï¼š",final_response)
        return final_response

    async def gain_all_tools_result(self, user_input: str) -> str:
        """ä¸Agentè¿›è¡Œå¯¹è¯ï¼Œå¹¶è¿”å›æ‰€æœ‰å·¥å…·è°ƒç”¨çš„ç»“æœ"""
        response = await self.agent_executor.ainvoke({
            "input": user_input
        })
        # print(response)
        # æ”¶é›†æ‰€æœ‰å·¥å…·è°ƒç”¨çš„ç»“æœ
        tool_results = []
        if response.get("intermediate_steps"):
            for action, observation in response["intermediate_steps"]:
                tool_name = action.tool
                tool_input = action.tool_input
                tool_output = observation  # è¿™æ˜¯å·¥å…·çš„ç›´æ¥è¿”å›ç»“æœ
                tool_results.append(
                    f"ğŸ”§ å·¥å…·è°ƒç”¨: {tool_name}\n"
                    f"ğŸ“Œ è¾“å…¥å‚æ•°: {tool_input}\n"
                    f"ğŸ“¢ è¿”å›ç»“æœ: {tool_output}\n"
                    "----------------------------------------\n"
                )

        # æ„å»ºæœ€ç»ˆè¾“å‡º
        final_response = ""
        if tool_results:
            final_response += "=== å·¥å…·è°ƒç”¨è®°å½• ===\n" + "\n".join(tool_results) + "\n\n"
        final_response += "=== Agent æœ€ç»ˆè¾“å‡º ===\n" + response["output"]
        # print(final_response)
        return final_response

    # å¸¦è®°å¿†çš„å‡½æ•°æ–¹æ³•
    async def talk_with_memory(self, user_input: str, session_id: str, file_paths: list[str] = None) -> str:
        """ä¸Agentè¿›è¡Œå¯¹è¯ï¼ˆæ–°å¢session_idå‚æ•°ï¼‰"""
        # ç”¨æˆ·è¾“å…¥çš„tokenéªŒè¯
        input_tokens = self._count_tokens(user_input)
        input_limited_tokens = 3000  # ç”¨æˆ·è¾“å…¥é™åˆ¶çš„æœ€å¤§tokenæ•°
        if input_tokens > input_limited_tokens:
            raise ValueError(
                f"è¾“å…¥å†…å®¹è¿‡é•¿ï¼ˆ{input_tokens} tokensï¼‰ã€‚"
                f"è¯·å°†å†…å®¹ç¼©çŸ­è‡³1500 tokensä»¥å†…ï¼ˆçº¦{input_limited_tokens * 0.8}ä¸ªè‹±æ–‡å•è¯æˆ–{input_limited_tokens //1.5 }ä¸ªä¸­æ–‡å­—ï¼‰ã€‚"
            )

        # å¤„ç†æ–‡ä»¶å†…å®¹ï¼ˆä½¿ç”¨ç°æœ‰çš„read_file_contentå‡½æ•°ï¼‰
        file_contents = []
        total_file_tokens = 0
        file_limited_tokens = 3000  # æ–‡ä»¶å†…å®¹é™åˆ¶çš„æœ€å¤§tokenæ•°

        if file_paths:
            for file_path in file_paths:
                try:
                    from tools.translations_tools import read_file_content
                    content = read_file_content(file_path)
                    content_tokens = self._count_tokens(content)

                    # æˆªæ–­è¶…é™å†…å®¹
                    remaining_tokens = file_limited_tokens - total_file_tokens
                    if content_tokens > remaining_tokens:
                        content = self._truncate_by_tokens(content, remaining_tokens)
                        # print(f"æ–‡ä»¶ {os.path.basename(file_path)} å†…å®¹è¶…å‡ºé™åˆ¶ï¼Œå·²æˆªæ–­")

                    file_contents.append(f"æ–‡ä»¶å†…å®¹({os.path.basename(file_path)}):\n{content}")
                    total_file_tokens += self._count_tokens(content)

                    if total_file_tokens >= file_limited_tokens:
                        # print(f"æ–‡ä»¶å†…å®¹å·²è¾¾tokenä¸Šé™ï¼ˆ{file_limited_tokens}ï¼‰")
                        break

                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")


        # æ„å»ºå®Œæ•´è¾“å…¥
        full_input = user_input
        if file_contents:
            full_input += "\n\n=== é™„åŠ æ–‡ä»¶å†…å®¹ ===\n" + "\n\n".join(file_contents)

        # ä»¥ä¸‹å®ç°å¯¹è¯
        config = RunnableConfig(configurable={"session_id": session_id})  # ä½¿ç”¨æ­£ç¡®çš„é…ç½®ç±»å‹ï¼ˆç”¨session_idç¡®å®šå¯¹è¯ï¼‰
        memory = self._get_summary_memory(session_id)

        history_summary = memory.moving_summary_buffer  # è·å–å†å²æ‘˜è¦
        # print("å†å²æ‘˜è¦ï¼š", history_summary)  # æ‰“å°å†å²æ‘˜è¦å†…å®¹ï¼Œä¾¿äºè°ƒè¯•
        response = await self.agent_memory_executor.ainvoke(
            {
                "input": full_input,
                "history_summary": history_summary  # å°†å†å²æ‘˜è¦ä¼ å…¥
            },
            config=config  # ä¼ å…¥æ­£ç¡®ç±»å‹çš„é…ç½®
        )
        # print("Response:", response)  # æ‰“å°å®Œæ•´çš„å“åº”å†…å®¹
        return response["output"]
        # literature_summary=""
        # # å¦‚æœæ˜¯æ–‡çŒ®æ€»ç»“å·¥å…·ï¼Œè®°å½•å·¥å…·è¿”å›ç»“æœ
        # if response.get("intermediate_steps"):
        #     for action,result in response["intermediate_steps"]:
        #         if action.tool== "LiteratureSummarizer":
        #             literature_summary += str(result)
        # # print("æ–‡çŒ®æ€»ç»“ç»“æœï¼š", literature_summary)
        # final_response = literature_summary+"\n\n" + response["output"]  # å°†æ€»ç»“ç»“æœå’Œæœ€ç»ˆè¾“å‡ºåˆå¹¶
        # # print("æœ€ç»ˆè¾“å‡ºï¼š",final_response)
        # return final_response

    async def clear_history(self, session_id: str):
        """æ¸…é™¤æŒ‡å®š session çš„å†å²å¯¹è¯"""
        file_path = self.MEMORY_DIR / f"{session_id}.json"
        if file_path.exists():
            os.remove(file_path)
